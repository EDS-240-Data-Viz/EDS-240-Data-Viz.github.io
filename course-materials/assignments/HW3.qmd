---
title: "HW #3: Visualizing FEMA NRI x ACS Data"
subtitle: "Assigned Wed 02/04/2026 | Due Wed 02/11/2026"
editor_options: 
  chunk_output_type: console
---

## Learning Outcomes {#LOs}

- identify which types of visualizations are most appropriate for your data and your audience
- prepare (e.g. clean, explore, wrangle) data so that it's appropriately formatted for building data visualizations
- build effective, responsible, accessible, and aesthetically-pleasing, visualizations using the R programming language, and specifically `{ggplot2}` + ggplot2 extension packages

## Description {#description}

[**In class, we've been discussing strategies and considerations for choosing the right graphic form to represent your data and convey your intended message. Here, you'll apply what we're learning to natural hazards and demographics data, courtesy of the [FEMA National Risk Index](https://hazards.fema.gov/nri/){target="_blank"} (NRI) and the [US Census Bureau's American Community Survey](https://www.census.gov/programs-surveys/acs/about.html){target="_blank"} (ACS).**]{.teal-text}

### I. Background reading {#partI}

[**Refamiliarize yourself with the FEMA's NRI and learn more about the ACS.**]{.teal-text} (unfold the following note, which is collapsed to save space):

::: {.callout-note collapse=true}
## About the data

### About FEMA's National Risk Index (NRI) for Natural Hazards *(repeated from HW #2)*

[FEMA](https://www.fema.gov/about){target="_blank"} (Federal Emergency Management Agency) is a government agency with a mission of helping people before, during, and after disasters. In 2021, FEMA launched the **[National Risk Index (NRI)](https://hazards.fema.gov/nri/){target="_blank"}**, "a [dataset](https://hazards.fema.gov/nri/data-resources){target="_blank"} and [online tool](https://hazards.fema.gov/nri/map){target="_blank"} to help illustrate the United States communities most at risk for [18 natural hazards](https://hazards.fema.gov/nri/natural-hazards){target="_blank"}".

**Risk** is defined as the potential for negative impacts resulting from natural hazards. It's calculated using the following equation (and illustrated in [this graphic](https://hazards.fema.gov/nri/Content/Images/StaticPageImages/Risk_Index_Calculation.svg){target="_blank"}; read more about [determining risk](https://hazards.fema.gov/nri/determining-risk){target="_blank"}):

$$Risk\:Index = Expected\:Annual\:Loss \times \frac{Social\:Vulnerability}{Community\:Resilience}$$

NRI provides hazard type-specific scores, as well as a composite score, which adds together the risk from all 18 hazard types. A community's **risk score** is represented by its percentile ranking among all other communities at the same level for Risk, Expected Annual Loss, Social Vulnerability and Community Resilience -- for example, if a given county's Risk Index percentile for a hazard type is 84.32 then its Risk Index value is greater than 84.32% of all US counties. Each community is also assigned a **risk rating**, which is a qualitative rating that describes the community in comparison to all other communities at the same level, ranging from “Very Low” to “Very High.”

You can learn more about the NRI at [hazards.fema.gov/nri](https://hazards.fema.gov/nri/learn-more){target="_blank"}.

```{r}
#| eval: true
#| echo: false
#| out-width: "80%"
#| fig-align: "center"
knitr::include_graphics("images/NRI.png")
```

::: {.center-text .gray-text .body-text-s}
*Screenshot of the [The National Risk Index's interactive mapping and data-based interface](https://hazards.fema.gov/nri/map)*
:::

#### Accessing NRI Data

Data at the county- and census tract-level are available for download in multiple formats (including Shapefiles & CSVs) from NRI's [Data Resources page](https://hazards.fema.gov/nri/data-resources){target="_blank"}.

### About the US Census Bureau's American Community Survey (ACS)

The **[American Community Survey (ACS)](https://www.census.gov/programs-surveys/acs/about.html){target="_blank"}** is a nationwide, continuous survey designed to provide communities with reliable and timely social, economic, housing, and demographic data every year. Unlike the **Decennial Census** (which counts every person in the US every 10 years for the purpose of congressional appointment), the ACS collects detailed information from a small subset of the population (~3.5 million households) at 1- and 5-year intervals. Learn more about the [differences between these 1- and 5-year estimates](https://www.census.gov/programs-surveys/acs/guidance/estimates.html){target="_blank"}.

#### Accessing ACS Data

The US Census Bureau provides a couple of tools for accessing their data, including: 

1. **[data.census.gov](https://data.census.gov){target="_blank"}:** a browser-based portal for exploring the many available [data tables](https://data.census.gov/table){target="_blank"} (e.g. [Table B02001](https://data.census.gov/table?q=B02001){target="_blank"}: Race)
2. **[The Census Data API](https://www.census.gov/data/developers/data-sets.html){target="_blank"}:** a data service that enables software developers to access and use Census Bureau data within their applications

*However,* when working in R, the **[`{tidycensus}`](https://walker-data.com/tidycensus/){target="_blank"} package** is arguably the easiest way to query and retrieve Census data -- use the [`get_acs()` function](https://walker-data.com/tidycensus/articles/basic-usage.html#working-with-acs-data){target="_blank"} to obtain ACS data for specified geographies (e.g. counties or census tracts), tables (e.g. B02001), variables (e.g. B02001_002, B02001_003), years (e.g. 2023), states (e.g. CA), surveys (e.g. acs1, acs5), etc.
:::

### II. Update your `eds240-nri-acs-viz` GitHub repo {#partII}

[**Make the following modifications to your `eds240-nri-acs-viz`repository:**]{.teal-text}

- [ ] update your README to reflect the new work you're adding (you can wait until the end of your assignment to complete this, if you prefer)
- [ ] add a `HW3.qmd` file with the necessary YAML 

### III. Import & save ACS data {#partIII}

[**Let's download the necessary data using the US Census Bureau's API via the `{tidycensus}` package and write a copy to file. We'll then read that saved copy of our data back in for use.**]{.teal-text} 

The following three steps describe the code you see in the following chunk. You may copy this code into the top of your `HW3.qmd` file, where you'd normally read in your data (remember to load any necessary packages first, though, and clean up the code comments as necessary).

1. Import ACS data using `tidycensus::get_acs()`: You'll need your API key to use `{tidycensus}` functions (revisit [week 2 pre-class prep instructions](../week2.qmd#download-templates){target="_blank"}, if necessary).

2. Write your data to `.csv`: It's always a good idea to write your data (i.e. the `race_ethnicity` data frame, from above) to file, in case the Census Bureau's API goes down. If you write your csv file to your `data/` folder, it will automatically be ignored by `.gitignore`.

3. Read in your saved `ACS-race-ethnicity.csv` file. 

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: true
#....Step 1a: see all available ACS variables + descriptions.....
acs_vars <- tidycensus::load_variables(year = 2023,
                                       dataset = "acs1")

#..............Step 1b: import race & ethnicity data.............
race_ethnicity <- tidycensus::get_acs(
  geography = "county",
  survey = "acs1",
  # NOTE: you may not end up using all these variables
  variables = c("B01003_001", "B02001_002", "B02001_003", 
                "B02001_004", "B02001_005", "B02001_006",
                "B02001_007", "B02001_008", "B03002_012",
                "B03002_002"),
  state = "CA", 
  year = 2023) |>
  # join variable descriptions (so we know what's what!)
  dplyr::left_join(acs_vars, by = dplyr::join_by(variable == name)) 

#.................Step 2: write ACS data to file.................
readr::write_csv(race_ethnicity, here::here("data", "ACS-race-ethnicity.csv"))

#..................Step 3: read in your CSV file.................
race_ethnicity <- readr::read_csv(here::here("data", "ACS-race-ethnicity.csv"))
```

4. At this point, you should comment out code from steps 1 & 2 (lines 1-20, above) so that you're not pinging the API & overwriting your saved copy each time you render `HW3.qmd`. Comment these lines out, rather that delete them altogether, so that anyone checking out your work understands how you initially retrieved the data.

### IV. Build your visualization {#partIV}

[**Create a data viz that helps to answer the question, *How does climate hazard risk exposure vary across racial / ethnic groups in California?***]{.teal-text} This will require some data wrangling first (including joining the NRI and ACS data).

::: {.callout-tip collapse="true"}
## REMINDER: before you start wrangling / building your viz, be sure to...

- **explore the metadata** -- `NRI_Table_Counties/` contains a metadata file named, `NRIDataDictionary.csv`, which describes each of the variables found in the data file (`NRI_Table_Counties.csv`). This is a helpful place to start!
- **identify / jot down your variables of interest** and consider which data types they are
- **use online tools like [from Data to Viz](https://www.data-to-viz.com/){target="_blank}** to help determine appropriate graph types, given your variables 
- **roughly sketch out your plots by hand** (see [why](https://medium.com/@tjanmichela/the-importance-of-sketching-in-data-visualization-78320c62e403){target="_blank"} this step matters!)
:::

Your final visualization should:

- [ ] include the following racial / ethnic groups: White, Black or African American, American Indian and Alaska Native, Asian, Native Hawaiian and Other Pacific Islander, Some Other Race, Two or More Races, Hispanic or Latino
- [ ] include a title (short, descriptive) & subtitle (describes main takeaway) (see [Fundamentals of Data Visualization, Ch 22](https://clauswilke.com/dataviz/figure-titles-captions.html){target="_blank"} for an example), a caption (describes the data source, e.g. "Data: FEMA National Risk Index (2023 Release)"), and alt text (following the formula discussed in [week 3 discussion](../discussion/week3/week3-slides.qmd){target="_blank"}; use the `fig-alt` code chunk option to apply your alt text)
- [ ] consider and implement strategies for highlighting trends / important information (e.g. arranging data, highlighting data, adjusting scales)
- [ ] use custom colors (if applicable), rather than ggplot defaults 
- [ ] have an updated theme / polished theme (revisit [week 4 discussion materials](../week4.qmd#discussion-materials){target="_blank"} for content on fine-tuning ggplot themes)

::: {.callout-tip}
## Is your plot looking squished in your rendered `.qmd` file? Adjust the aspect ratio!
You can adjust the aspect ratio of your plot (i.e. its width to height ratio) so that your data / groups are easy to read. Add the `fig-asp` option to your code chunk YAML, which makes adjusting aspect ratios for rendered outputs quite easy. Values > 1 make your plot taller and values < 1 make your plot wider. 
:::

### V. Answer some questions    

- [ ] **1.** What are your variables of interest and what kinds of data (e.g. numeric, categorical, ordered, etc.) are they (a bullet point list is fine)?

- [ ] **2.** How did you decide which type of graphic form was best suited for answering the question? What alternative graphic forms could you have used instead? Why did you settle on this particular graphic form?

- [ ] **3.** Summarize your main finding in no more than two sentences.

- [ ] **4.** What modifications did you make to this visualization to make it more easily readable?

- [ ] **5.** Is there anything you wanted to implement, but didn't know how? If so, please describe.

## Rubric (specifications) {#rubric}

To recieve a "Satisfactory" on HW #3:

- [ ] **Create one visualization** that answers the question, *"How does climate hazard risk exposure vary across racial / ethnic groups in California?"* and fulfills the requirements listed in Part IV.
- [ ] **All code should follow appropriate styling conventions and include clear, informative annotations.** We will not enforce any particular code style guide (you may use conventions from prior courses if you prefer; we recommend following [these conventions](../../clean-code-guide.qmd){target="_blank"} from the [Tidyverse style guide](https://style.tidyverse.org/index.html){target="_blank"} if you're unsure where to start). You don’t need to explain every line, but your annotations should make it easy to follow your thinking and understand important decisions in your code.
- [ ] **Answer all three Part V questions.** There are no strict length requirements for free-response questions, however we expect that you answer them thoughtfully and fully.
- [ ] **Ensure that everything described above is included in your neatly-organized and polished `HW3.qmd` file** -- this includes appropriate document YAML (at a minimum, title, author, date), clearly labeled sections, appropriately-formatted prose, appropriate code chunk options (e.g. code should render and execute, but warnings and messages should be suppressed, long data frames should not be printed out, etc.)
- [ ] **All your work is housed in your organized and polished `eds240-nri-acs-viz` GitHub repository.** Your README is updated to include any new relevant information. You've verified that your new ACS data are saved to your `data/` folder, which is ignored by `.gitignore`.
- [ ] **Render `HW3.qmd` as a PDF.** Double check that all of your outputs, code, and prose are visible.
- [ ] **Upload your PDF to Gradescope** by 11:59pm on Wed 02/11/2026.
- [ ] **Submit a [Generative AI Statement of Use](https://forms.gle/uCw7U5jzJAKahjA4A){target="_blank}** for HW #3 by 11:59pm on Wed 02/11/2026.



